# üîç OpsVision

> **AI-Powered Real-Time Event Intelligence Platform**

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![React](https://img.shields.io/badge/React-19-61DAFB?logo=react)](https://react.dev/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com)
[![Confluent](https://img.shields.io/badge/Confluent-Kafka-FF6B35?logo=apache-kafka)](https://www.confluent.io/)
[![Flink](https://img.shields.io/badge/Apache%20Flink-SQL-E6526F?logo=apache-flink)](https://flink.apache.org)
[![Gemini](https://img.shields.io/badge/Google-Gemini-4285F4?logo=google)](https://ai.google.dev)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

OpsVision transforms raw operational events from multiple sources (GitHub, Kubernetes, Datadog, Jenkins, PagerDuty) into actionable intelligence using **Apache Kafka** for real-time streaming, **Apache Flink SQL** for stream processing, and **Google Gemini AI** for intelligent insights.

**üèÜ Built for [AI Partner Catalyst: Accelerate Innovation](https://ai-partner-catalyst.devpost.com/) Hackathon**

---

## üìã Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Project Structure](#-project-structure)
- [Prerequisites](#-prerequisites)
- [Quick Start](#-quick-start)
  - [1. Clone Repository](#1-clone-repository)
  - [2. Backend Setup](#2-backend-setup)
  - [3. Frontend Setup](#3-frontend-setup)
  - [4. Flink SQL Setup](#4-flink-sql-setup)
- [Environment Configuration](#-environment-configuration)
- [Running the Application](#-running-the-application)
- [API Reference](#-api-reference)
- [Demo Scenarios](#-demo-scenarios)
- [Testing](#-testing)
- [Tech Stack](#-tech-stack)
- [License](#-license)

---

## ‚ú® Features

| Feature | Description |
|---------|-------------|
| üöÄ **Real-Time Streaming** | CloudEvents ingestion via Confluent Kafka with Avro serialization |
| ü§ñ **AI-Powered Insights** | Google Gemini generates actionable recommendations from event patterns |
| üìä **Live Dashboard** | React-based real-time dashboard with WebSocket updates |
| üîó **Event Correlation** | Flink SQL aggregates and correlates events across 5-minute windows |
| üìà **System Health Monitoring** | Automatic health status classification (CRITICAL/DEGRADED/WARNING/HEALTHY) |
| üé≠ **Scenario Simulation** | Built-in scenarios for demos: Normal Ops, Deployment, Incident, Traffic Spike |
| üîå **Multi-Source Integration** | GitHub, Datadog, Kubernetes, Jenkins, PagerDuty webhook adapters |

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         EVENT SOURCES                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  GitHub  ‚îÇ Datadog  ‚îÇ Kubernetes‚îÇ Jenkins  ‚îÇ     PagerDuty        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ          ‚îÇ           ‚îÇ          ‚îÇ                ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 FastAPI Backend (Event Collector)                     ‚îÇ
‚îÇ                 Transforms webhooks ‚Üí CloudEvents format              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ Avro
                                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 Confluent Cloud Kafka                                 ‚îÇ
‚îÇ                 (cloudevents-stream topic)                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
                                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 Apache Flink SQL Pipeline                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Source Table   ‚îÇ‚Üí ‚îÇ 5-min Aggregation‚îÇ‚Üí ‚îÇ Health + Correlation ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                      ‚îÇ               ‚îÇ
‚îÇ                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ                              ‚ñº                                        ‚îÇ
‚îÇ                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                              ‚îÇ
‚îÇ                     ‚îÇ gemini_summary   ‚îÇ                              ‚îÇ
‚îÇ                     ‚îÇ (AI-ready output)‚îÇ                              ‚îÇ
‚îÇ                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚ñº                ‚ñº                ‚ñº
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ Google      ‚îÇ   ‚îÇ FastAPI      ‚îÇ   ‚îÇ React        ‚îÇ
     ‚îÇ Gemini AI   ‚îÇ‚Üí  ‚îÇ WebSocket    ‚îÇ‚Üí  ‚îÇ Dashboard    ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÅ Project Structure

```
OpsVision/
‚îú‚îÄ‚îÄ python-backend/          # FastAPI backend server
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py        # Configuration & environment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py        # Pydantic models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes/          # API endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/        # Kafka, AI, WebSocket services
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas/         # Avro schemas
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # Entry point
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt     # Python dependencies
‚îÇ   ‚îî‚îÄ‚îÄ .env.example         # Environment template
‚îÇ
‚îú‚îÄ‚îÄ react-frontend/          # React dashboard
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/      # UI components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hooks/           # Custom React hooks
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ App.jsx          # Main application
‚îÇ   ‚îú‚îÄ‚îÄ package.json         # Node dependencies
‚îÇ   ‚îî‚îÄ‚îÄ .env.example         # Environment template
‚îÇ
‚îú‚îÄ‚îÄ flink-ksql/              # Confluent Flink SQL scripts
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ       ‚îú‚îÄ‚îÄ 01_source_table_cloudevents.sql
‚îÇ       ‚îú‚îÄ‚îÄ 02_events_aggregated_5min.sql
‚îÇ       ‚îú‚îÄ‚îÄ 03_system_health_5min.sql
‚îÇ       ‚îú‚îÄ‚îÄ 04_top_error_sources.sql
‚îÇ       ‚îú‚îÄ‚îÄ 05_correlated_incidents.sql
‚îÇ       ‚îú‚îÄ‚îÄ 08_gemini_summary.sql
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ README.md                # This file
‚îî‚îÄ‚îÄ LICENSE                  # MIT License
```

---

## üìã Prerequisites

- **Python 3.11+** - [Download](https://python.org)
- **Node.js 20+** - [Download](https://nodejs.org)
- **Confluent Cloud Account** - [Sign Up](https://confluent.io)
  - Kafka cluster
  - Flink compute pool (2+ CFUs recommended)
  - Schema Registry
- **Google Gemini API Key** - [Get API Key](https://ai.google.dev)

---

## üöÄ Quick Start

### 1. Clone Repository

```bash
git clone https://github.com/sachin-handiekar/OpsVision.git
cd OpsVision
```

### 2. Backend Setup

```bash
cd python-backend

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows:
.venv\Scripts\activate
# Linux/Mac:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your credentials (see Environment Configuration below)
```

### 3. Frontend Setup

```bash
cd react-frontend

# Install dependencies
npm install

# Configure environment
cp .env.example .env
# Edit .env if using different backend URL
```

### 4. Flink SQL Setup

Execute the SQL files in order in the **Confluent Cloud Flink SQL Editor**:

1. **Create Source Table**: `01_source_table_cloudevents.sql`
2. **Create Aggregation Table**: `02_events_aggregated_5min.sql`
3. **Create Derived Tables** (can run in parallel):
   - `03_system_health_5min.sql`
   - `04_top_error_sources.sql`
   - `05_correlated_incidents.sql`
4. **Create AI Summary Table**: `08_gemini_summary.sql`

> **Note**: Each INSERT statement runs as a continuous streaming job in Confluent Cloud.

---

## ‚öôÔ∏è Environment Configuration

### Backend (`python-backend/.env`)

```env
# Kafka Configuration (Confluent Cloud)
KAFKA_BOOTSTRAP_SERVERS=your-cluster.region.gcp.confluent.cloud:9092
KAFKA_CLIENT_ID=opsvision-event-collector
KAFKA_EVENTS_TOPIC=cloudevents-stream
GEMINI_SUMMARY_TOPIC=gemini_summary

# Kafka Authentication
KAFKA_API_KEY=your-kafka-api-key
KAFKA_API_SECRET=your-kafka-api-secret

# Schema Registry
SCHEMA_REGISTRY_URL=https://your-sr.region.gcp.confluent.cloud
SCHEMA_REGISTRY_API_KEY=your-sr-api-key
SCHEMA_REGISTRY_API_SECRET=your-sr-api-secret

# Google Gemini AI
GEMINI_API_KEY=your-gemini-api-key
```

### Frontend (`react-frontend/.env`)

```env
VITE_API_URL=http://localhost:8000
VITE_WS_URL=ws://localhost:8000/ws
```

---

## üèÉ Running the Application

### Start Backend Server

```bash
cd python-backend
python main.py
```

Server starts at `http://localhost:8000`

### Start Frontend Development Server

```bash
cd react-frontend
npm run dev
```

Dashboard available at `http://localhost:5173`

### Verify Everything is Connected

1. Open the React dashboard in your browser
2. WebSocket connection indicator should show "Connected"
3. Use scenario simulation to test event flow

---

## üì° API Reference

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | Service info and health check |
| `GET` | `/api/stats` | Current event statistics |
| `GET` | `/api/templates` | Available event templates |
| `GET` | `/api/summaries` | Fetch AI summaries from Kafka |
| `POST` | `/api/simulate` | Send custom event to Kafka |
| `POST` | `/api/scenario/{name}` | Run predefined scenario |
| `WS` | `/ws` | Real-time WebSocket connection |

### Example: Simulate an Event

```bash
curl -X POST http://localhost:8000/api/simulate \
  -H "Content-Type: application/json" \
  -d '{"event_type": "error", "source": "api-gateway", "count": 5}'
```

---

## üé≠ Demo Scenarios

Trigger pre-built scenarios via API or the dashboard:

| Scenario | Description | Events |
|----------|-------------|--------|
| `normal` | Steady stream of info-level events | ~20 events |
| `deployment` | CI/CD pipeline flow (build‚Üídeploy‚Üíverify) | ~15 events |
| `incident` | Production incident with critical alerts | ~25 events |
| `traffic_spike` | High-volume traffic increase simulation | ~30 events |

**Run via API:**
```bash
curl -X POST http://localhost:8000/api/scenario/incident
```

**Run via Dashboard:**
Click scenario buttons in the Scenario Panel section.

---

## üß™ Testing

### Backend Tests

```bash
cd python-backend

# Run all tests
pytest tests/ -v

# With coverage report
pytest tests/ --cov=app --cov-report=html
```

### Frontend Build

```bash
cd react-frontend

# Lint code
npm run lint

# Build for production
npm run build
```

---

## üõ†Ô∏è Tech Stack

### Backend
- **Framework**: FastAPI, Uvicorn
- **Streaming**: Apache Kafka (Confluent Cloud)
- **Serialization**: Avro + Schema Registry
- **AI**: Google Gemini 2.0 Flash
- **Real-time**: WebSockets

### Frontend
- **Framework**: React 19
- **Build Tool**: Vite 7
- **Styling**: TailwindCSS 3
- **Icons**: Lucide React

### Stream Processing
- **Engine**: Apache Flink (Confluent Cloud)
- **Language**: Flink SQL
- **Event Format**: CloudEvents 1.0

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<p align="center">
  <b>‚≠ê Star this repo if you find it useful!</b>
  <br><br>
  Made with ‚ù§Ô∏è using Confluent Kafka & Google Gemini
</p>
